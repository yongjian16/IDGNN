{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.nn import Parameter\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def get_spectral_rad(sparse_tensor, tol=1e-5):\n",
    "    \"\"\"Compute spectral radius from a tensor\"\"\"\n",
    "    A = sparse_tensor.data.coalesce().cpu()\n",
    "    A_scipy = sp.coo_matrix((np.abs(A.values().numpy()), A.indices().numpy()), shape=A.shape)\n",
    "    return np.abs(sp.linalg.eigs(A_scipy, k=1, return_eigenvectors=False)[0]) + tol\n",
    "\n",
    "def projection_norm_inf(A, kappa=0.99, transpose=False):\n",
    "    \"\"\" project onto ||A||_inf <= kappa return updated A\"\"\"\n",
    "    # TODO: speed up if needed\n",
    "    v = kappa\n",
    "    if transpose:\n",
    "        A_np = A.T.clone().detach().cpu().numpy()\n",
    "    else:\n",
    "        A_np = A.clone().detach().cpu().numpy()\n",
    "    x = np.abs(A_np).sum(axis=-1)\n",
    "    for idx in np.where(x > v)[0]:\n",
    "        # read the vector\n",
    "        a_orig = A_np[idx, :]\n",
    "        a_sign = np.sign(a_orig)\n",
    "        a_abs = np.abs(a_orig)\n",
    "        a = np.sort(a_abs)\n",
    "\n",
    "        s = np.sum(a) - v\n",
    "        l = float(len(a))\n",
    "        for i in range(len(a)):\n",
    "            # proposal: alpha <= a[i]\n",
    "            if s / l > a[i]:\n",
    "                s -= a[i]\n",
    "                l -= 1\n",
    "            else:\n",
    "                break\n",
    "        alpha = s / l\n",
    "        a = a_sign * np.maximum(a_abs - alpha, 0)\n",
    "        # verify\n",
    "        assert np.isclose(np.abs(a).sum(), v, atol=1e-4)\n",
    "        # write back\n",
    "        A_np[idx, :] = a\n",
    "    A.data.copy_(torch.tensor(A_np.T if transpose else A_np, dtype=A.dtype, device=A.device))\n",
    "    return A\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx, device=None):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    tensor = torch.sparse.FloatTensor(indices, values, shape)\n",
    "    if device is not None:\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor\n",
    "\n",
    "def fetch_normalization(type):\n",
    "   switcher = {\n",
    "       'AugNormAdj': aug_normalized_adjacency,  # A' = (D + I)^-1/2 * ( A + I ) * (D + I)^-1/2\n",
    "   }\n",
    "   func = switcher.get(type, lambda: \"Invalid normalization technique.\")\n",
    "   return func\n",
    "\n",
    "def aug_normalized_adjacency(adj, need_orig=False):\n",
    "   if not need_orig:\n",
    "       adj = adj + sp.eye(adj.shape[0])\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   row_sum = np.array(adj.sum(1))\n",
    "   d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "   d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "   d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "   return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def row_normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def preprocess_citation(adj, features, normalization=\"FirstOrderGCN\"):\n",
    "    adj_normalizer = fetch_normalization(normalization)\n",
    "    adj = adj_normalizer(adj)\n",
    "    features = row_normalize(features)\n",
    "    return adj, features\n",
    "\n",
    "\n",
    "def load_citation_chain(normalization, cuda, need_orig=False):\n",
    "    \"\"\"load the synthetic dataset: chain\"\"\"\n",
    "    r = np.random.RandomState(42)\n",
    "    c = 2 # num of classes\n",
    "    n = 20 # chains for each class\n",
    "    l = 10 # length of chain\n",
    "    f = 100 # feature dimension\n",
    "    tn = 20  # train nodes\n",
    "    vl = 100 # val nodes\n",
    "    tt = 200 # test nodes\n",
    "    noise = 0.00\n",
    "\n",
    "    \n",
    "    chain_adj = sp.coo_matrix((np.ones(l-1), (np.arange(l-1), np.arange(1, l))), shape=(l, l))\n",
    "    adj = sp.block_diag([chain_adj for _ in range(c*n)]) # square matrix N = c*n*l\n",
    "\n",
    "    features = r.uniform(-noise, noise, size=(c, n, l, f))\n",
    "    #features = np.zeros_like(features)\n",
    "    features[:, :, 0, :c] += np.eye(c).reshape(c, 1, c) # add class info to the first node of chains.\n",
    "    features = features.reshape(-1, f)\n",
    "\n",
    "    labels = np.eye(c).reshape(c, 1, 1, c).repeat(n, axis=1).repeat(l, axis=2) # one-hot labels\n",
    "    labels = labels.reshape(-1, c)\n",
    "\n",
    "    idx_random = np.arange(c*n*l)\n",
    "    r.shuffle(idx_random)\n",
    "    idx_train = idx_random[:tn]\n",
    "    idx_val = idx_random[tn:tn+vl]\n",
    "    idx_test = idx_random[tn+vl:tn+vl+tt]\n",
    "\n",
    "    if need_orig:\n",
    "        adj_orig = aug_normalized_adjacency(adj, need_orig=True)\n",
    "        adj_orig = sparse_mx_to_torch_sparse_tensor(adj_orig).float()\n",
    "        if cuda:\n",
    "            adj_orig = adj_orig.cuda()\n",
    "\n",
    "    adj, features = preprocess_citation(adj, features, normalization)\n",
    "\n",
    "    # porting to pytorch\n",
    "    features = torch.FloatTensor(np.array(features.todense() if sp.issparse(features) else features)).float()\n",
    "    labels = torch.LongTensor(labels)\n",
    "    labels = torch.max(labels, dim=1)[1]\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    if cuda:\n",
    "        features = features.cuda()\n",
    "        adj = adj.cuda()\n",
    "        labels = labels.cuda()\n",
    "        idx_train = idx_train.cuda()\n",
    "        idx_val = idx_val.cuda()\n",
    "        idx_test = idx_test.cuda()\n",
    "\n",
    "    return [adj, adj_orig] if need_orig else adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "class bimodel(torch.nn.Module):\n",
    "    def __init__(self, num_in, num_hid, num_out, num_node, time_steps, kappa=0.99, phi=F.relu, b_direct=False):\n",
    "        super(bimodel, self).__init__()\n",
    "        self.i = num_in\n",
    "        self.h = num_hid\n",
    "        self.o = num_out\n",
    "        self.n = num_node\n",
    "        self.t = time_steps\n",
    "        self.k = kappa\n",
    "        self.direct = b_direct\n",
    "\n",
    "        self.phi = [F.relu]*self.t\n",
    "        self.X_0 = Parameter(torch.zeros(self.h, num_node), requires_grad=False)\n",
    "        self.W = nn.ParameterList([Parameter(torch.FloatTensor(self.h, self.h)) for i in range(self.t)])\n",
    "#         self.W = nn.ParameterList([Parameter(torch.FloatTensor(self.h, self.h))])\n",
    "#         self.Omega = nn.ParameterList([Parameter(torch.FloatTensor(self.h, self.i)) for i in range(self.t)])\n",
    "        self.V = Parameter(torch.FloatTensor(self.h, self.i))\n",
    "#         self.linear = nn.Linear(self.h, self.o)\n",
    "        self.classifier = nn.Sequential(\n",
    "#                                     nn.Dropout(p=0.3)\n",
    "                                    # nn.Linear(self.h, self.h),\n",
    "#                                     nn.BatchNorm1d(self.h),\n",
    "                                    # nn.Softplus(),\n",
    "                                    nn.Linear(self.h, self.o),\n",
    "                                    nn.LogSoftmax(dim=1)\n",
    "                                    )\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "#         stdv = 0.01\n",
    "        for i in range(len(self.W)):\n",
    "            stdv = 1. / (math.sqrt(self.W[i].size(1))* self.t)\n",
    "            self.W[i].data.uniform_(0, stdv)\n",
    "#             self.Omega[i].data.uniform_(-stdv, stdv)\n",
    "#         stdv = 1. / self.W[0].size(1)\n",
    "#         self.W[0].data.uniform_(-stdv, stdv)\n",
    "        stdv = 1. /(math.sqrt(self.V.size(1))* self.t) \n",
    "        # stdv = 1\n",
    "        self.V.data.uniform_(0, stdv)\n",
    "        \n",
    "    def project(self,X_list, A_list, A_rho):\n",
    "        \n",
    "        self.X_list = X_list\n",
    "        self.A_list = A_list\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] = projection_norm_inf(self.W[i], kappa=self.k / A_rho[i])\n",
    "            \n",
    "        \n",
    "    def forward(self,Z):\n",
    "\n",
    "        X_list = self.X_list\n",
    "        A_list = self.A_list\n",
    "        V = self.V\n",
    "        W_list = self.W\n",
    "        phi = F.relu\n",
    "\n",
    "        for j in range(len(A_list)):\n",
    "                W = W_list[j]\n",
    "                A = A_list[j]\n",
    "                B = torch.spmm(V, X_list[j])\n",
    "                Z_ = W @ Z\n",
    "                support = torch.spmm(A.T, Z_.T).T\n",
    "\n",
    "                Z = phi(support + B)\n",
    "        return Z\n",
    "    \n",
    "    def predict(self, X_list, A_list):\n",
    "        # for i in range(len(self.W)):\n",
    "        #     self.W[i] = projection_norm_inf(self.W[i], kappa=self.k / A_rho[i])\n",
    "        \n",
    "        V = self.V\n",
    "        W_list = self.W\n",
    "        phi = F.relu\n",
    "        max_iter = 300\n",
    "        tol = 1e-6\n",
    "        \n",
    "        device = W_list[0].device\n",
    "        Z = torch.zeros(self.X_0.shape).to(device)\n",
    "        \n",
    "        status = 'max itrs reached'\n",
    "        for i in range(max_iter):\n",
    "            Z_old = Z\n",
    "            for j in range(len(A_list)):\n",
    "                    W = W_list[j]\n",
    "                    A = A_list[j]\n",
    "                    B = torch.spmm(V, X_list[j])\n",
    "                    Z_ = W @ Z_old\n",
    "                    support = torch.spmm(A.T, Z_.T).T\n",
    "\n",
    "                    Z = phi(support + B)\n",
    "            \n",
    "            err = torch.norm(Z - Z_old, np.inf)\n",
    "            if err < tol:\n",
    "                status = 'converged'\n",
    "                break\n",
    "            \n",
    "        if status == 'max itrs reached':\n",
    "                \n",
    "                print('Forward Not Converge! Error: %3.5f, tol: %3.5f' % (err, tol))\n",
    "            \n",
    "        Y_hat = self.classifier(Z.T)\n",
    "        return Y_hat, Z.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_time_chain():\n",
    "    r = np.random.RandomState(42)\n",
    "    c = 2 # num of classes\n",
    "    nc = 10 # num of each class\n",
    "    n = 10 # nodes of chain\n",
    "    l = 10 # length of chain\n",
    "    f = 100 # feature dimension\n",
    "    tn = 2  # train nodes\n",
    "    vl = 1 # val nodes\n",
    "    tt = 7 # test nodes\n",
    "    noise = 0.00\n",
    "\n",
    "    X_list = []\n",
    "    A_list = []\n",
    "    Y_list = []\n",
    "    E_list = []\n",
    "    for i in range(nc):\n",
    "        features = r.uniform(-noise, noise, size=(l, n, f))\n",
    "        features[0,0,0] += 1 \n",
    "        X = [torch.tensor(features[j,:,:]) for j in range(l)]\n",
    "        labels = torch.ones(n, dtype=torch.long)\n",
    "        \n",
    "        edge_index = torch.tensor(np.array([np.arange(10-1), np.arange(1, 10)]))\n",
    "        E = [edge_index] * l\n",
    "        chain_adj = sp.coo_matrix((np.ones(n-1), (np.arange(n-1), np.arange(1, n))), shape=(n, n))\n",
    "        chain_adj = aug_normalized_adjacency(chain_adj)\n",
    "        A = [chain_adj] * l\n",
    "\n",
    "        X_list.append(X)\n",
    "        A_list.append(A)\n",
    "        Y_list.append(labels)\n",
    "        E_list.append(E)\n",
    "\n",
    "    for i in range(nc):\n",
    "        features = r.uniform(-noise, noise, size=(l, n, f))\n",
    "        X = [torch.tensor(features[j,:,:]) for j in range(l)]\n",
    "        labels = torch.zeros(n, dtype=torch.long)\n",
    "\n",
    "        edge_index = torch.tensor(np.array([np.arange(10-1), np.arange(1, 10)]))\n",
    "        E = [edge_index] * l\n",
    "        chain_adj = sp.coo_matrix((np.ones(n-1), (np.arange(n-1), np.arange(1, n))), shape=(n, n))\n",
    "        chain_adj = aug_normalized_adjacency(chain_adj)\n",
    "        A = [chain_adj] * l\n",
    "\n",
    "        X_list.append(X)\n",
    "        A_list.append(A)\n",
    "        Y_list.append(labels)\n",
    "        E_list.append(E)\n",
    "\n",
    "    return X_list, A_list, Y_list, E_list\n",
    "            \n",
    "def load_time_chain2(l=10):\n",
    "    r = np.random.RandomState(42)\n",
    "    c = 2 # num of classes\n",
    "    nc = 20 # num of each class\n",
    "    n = 10 # nodes of chain\n",
    "    # l = 5 # length of chain\n",
    "    f = 4 # feature dimension\n",
    "    tn = 20  # train nodes\n",
    "    vl = 100 # val nodes\n",
    "    tt = 200 # test nodes\n",
    "    noise = 0.00\n",
    "\n",
    "    X_list = []\n",
    "    A_list = []\n",
    "    Y_list = []\n",
    "    E_list = []\n",
    "    \n",
    "    chain_adj = sp.coo_matrix((np.ones(n-1), (np.arange(n-1), np.arange(1, n))), shape=(n, n))\n",
    "    adj = sp.block_diag([chain_adj for _ in range(c*nc)]) # square matrix N = c*n*l\n",
    "\n",
    "    features = r.uniform(-noise, noise, size=(c, nc, n, f))\n",
    "    #features = np.zeros_like(features)\n",
    "    features[:, :, 0, :c] += np.eye(c).reshape(c, 1, c) # add class info to the first node of chains.\n",
    "    features = features.reshape(-1, f)\n",
    "\n",
    "    labels = np.eye(c).reshape(c, 1, 1, c).repeat(nc, axis=1).repeat(n, axis=2) # one-hot labels\n",
    "    labels = labels.reshape(-1, c)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    labels = torch.max(labels, dim=1)[1]\n",
    "    \n",
    "    adj, features = preprocess_citation(adj, features, \"AugNormAdj\")\n",
    "    X_list.append(features)\n",
    "    A_list.append(adj)\n",
    "    Y_list.append(labels)\n",
    "    \n",
    "    idx_random = np.arange(c*nc*n)\n",
    "    r.shuffle(idx_random)\n",
    "    idx_train = idx_random[:tn]\n",
    "    idx_val = idx_random[tn:tn+vl]\n",
    "    idx_test = idx_random[tn+vl:tn+vl+tt]\n",
    "    \n",
    "    for i in range(l-1):\n",
    "        chain_adj = sp.coo_matrix((np.ones(n-1), (np.arange(n-1), np.arange(1, n))), shape=(n, n))\n",
    "        adj = sp.block_diag([chain_adj for _ in range(c*nc)]) # square matrix N = c*n*l\n",
    "\n",
    "        features = r.uniform(-noise, noise, size=(c, nc, n, f))\n",
    "        features = features.reshape(-1, f)\n",
    "\n",
    "        adj, features = preprocess_citation(adj, features, \"AugNormAdj\")\n",
    "        X_list.append(features)\n",
    "        A_list.append(adj)\n",
    "        \n",
    "\n",
    "    return X_list, A_list, Y_list, idx_train, idx_val, idx_test\n",
    "\n",
    "def load_time_chain3(l=10):\n",
    "    r = np.random.RandomState(42)\n",
    "    c = 2 # num of classes\n",
    "    nc = 1 # num of each class\n",
    "    n = 200 # nodes of chain\n",
    "    # l = 5 # length of chain\n",
    "    f = 10 # feature dimension\n",
    "    tn = 200  # train nodes\n",
    "    vl = 100 # val nodes\n",
    "    tt = 100 # test nodes\n",
    "    noise = 0.01\n",
    "\n",
    "    X_list = []\n",
    "    A_list = []\n",
    "    Y_list = []\n",
    "    E_list = []\n",
    "    \n",
    "    \n",
    "    adj = sp.coo_matrix((np.ones(c*n-1), (np.arange(c*n-1), np.arange(1, c*n))), shape=(c*n, c*n))\n",
    "    # adj = sp.block_diag([chain_adj for _ in range(c*nc)]) # square matrix N = c*n*l\n",
    "\n",
    "    features = r.uniform(-0, 0, size=(c, nc, n, f))\n",
    "    #features = np.zeros_like(features)\n",
    "    features[:, 0, :, :c] += np.eye(c).reshape(c, 1, c) # add class info to the first node of chains.\n",
    "    features = features.reshape(-1, f)\n",
    "    \n",
    "\n",
    "    labels = np.eye(c).reshape(c, 1, 1, c).repeat(nc, axis=1).repeat(n, axis=2) # one-hot labels\n",
    "    labels = labels.reshape(-1, c)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    labels = torch.max(labels, dim=1)[1]\n",
    "    \n",
    "    adj, features = preprocess_citation(adj, features, \"AugNormAdj\")\n",
    "    X_list.append(features)\n",
    "    A_list.append(adj)\n",
    "    Y_list.append(labels)\n",
    "    \n",
    "    idx_random = np.arange(c*nc*n)\n",
    "    r.shuffle(idx_random)\n",
    "    idx_train = idx_random[:tn]\n",
    "    idx_val = idx_random[tn:tn+vl]\n",
    "    idx_test = idx_random[tn+vl:tn+vl+tt]\n",
    "    \n",
    "    for i in range(l-1):\n",
    "        adj = sp.coo_matrix((np.ones(c*n-1), (np.arange(c*n-1), np.arange(1, c*n))), shape=(c*n, c*n))\n",
    "        # adj = sp.block_diag([chain_adj for _ in range(c*nc)]) # square matrix N = c*n*l\n",
    "\n",
    "        features = noise*r.uniform(-noise, noise, size=(c, nc, n, f))\n",
    "\n",
    "        features = features.reshape(-1, f)\n",
    "\n",
    "        adj, features = preprocess_citation(adj, features, \"AugNormAdj\")\n",
    "        X_list.append(features)\n",
    "        A_list.append(adj)\n",
    "        \n",
    "\n",
    "    return X_list, A_list, Y_list, idx_train, idx_val, idx_test\n",
    "\n",
    "def clique_edge_index_ud(n):\n",
    "    rows, cols = np.meshgrid(np.arange(n), np.arange(n))\n",
    "    upper_tri_mask = np.triu(np.ones((n, n)), k=1).astype(bool)\n",
    "    lower_tri_mask = np.tril(np.ones((n, n)), k=-1).astype(bool)\n",
    "    rows = np.concatenate([rows[upper_tri_mask], rows[lower_tri_mask]], axis=0)\n",
    "    cols = np.concatenate([cols[upper_tri_mask], cols[lower_tri_mask]], axis=0)\n",
    "    return (rows, cols)\n",
    "\n",
    "def load_toy_data(l=10):\n",
    "    r = np.random.RandomState(42)\n",
    "    c = 2 # num of classes\n",
    "    nc = 1 # num of each class\n",
    "    n = 5 # nodes of chain\n",
    "    # l = 5 # length of chain\n",
    "    f = c*n # feature dimension\n",
    "    tn = 2  # train nodes\n",
    "    vl = 1 # val nodes\n",
    "    tt = 7 # test nodes\n",
    "    noise = 1\n",
    "\n",
    "    X_list = []\n",
    "    A_list = []\n",
    "    Y_list = []\n",
    "    E_list = []\n",
    "    \n",
    "    rows, cols = clique_edge_index_ud(c*n)\n",
    "    adj = sp.coo_matrix((np.ones(len(rows)), (rows, cols)), shape=(c*n, c*n))\n",
    "    features = np.eye(c*n)\n",
    "    \n",
    "    labels = np.eye(c*n)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    labels = torch.max(labels, dim=1)[1]\n",
    "    \n",
    "    adj, features = preprocess_citation(adj, features, \"AugNormAdj\")\n",
    "    X_list.append(features)\n",
    "    A_list.append(adj)\n",
    "    Y_list.append(labels)\n",
    "    \n",
    "    idx_random = np.arange(c*nc*n)\n",
    "    r.shuffle(idx_random)\n",
    "    idx_train = idx_random[:tn]\n",
    "    idx_val = idx_random[tn:tn+vl]\n",
    "    idx_test = idx_random[tn+vl:tn+vl+tt]\n",
    "    \n",
    "    for i in range(l-1):\n",
    "        features = noise*np.random.rand(c, nc, n, f)\n",
    "\n",
    "        features = features.reshape(-1, f)\n",
    "\n",
    "        adj, features = preprocess_citation(adj, features, \"AugNormAdj\")\n",
    "        X_list.append(features)\n",
    "        A_list.append(adj)\n",
    "        \n",
    "\n",
    "    return X_list, A_list, Y_list, idx_train, idx_val, idx_test\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def clique_edge_index(n):\n",
    "    rows, cols = np.meshgrid(np.arange(n), np.arange(n))\n",
    "    upper_tri_mask = np.triu(np.ones((n, n)), k=1).astype(bool)\n",
    "    return (rows[upper_tri_mask], cols[upper_tri_mask])\n",
    "\n",
    "def clique_edge_index_ud(n):\n",
    "    rows, cols = np.meshgrid(np.arange(n), np.arange(n))\n",
    "    upper_tri_mask = np.triu(np.ones((n, n)), k=1).astype(bool)\n",
    "    lower_tri_mask = np.tril(np.ones((n, n)), k=-1).astype(bool)\n",
    "    rows = np.concatenate([rows[upper_tri_mask], rows[lower_tri_mask]], axis=0)\n",
    "    cols = np.concatenate([cols[upper_tri_mask], cols[lower_tri_mask]], axis=0)\n",
    "    return (rows, cols)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from scipy.sparse import coo_array\n",
    "import time\n",
    "\n",
    "device = 'cuda:7'\n",
    "X_list, A_list, Y, idx_train, idx_val, idx_test = load_toy_data(10)\n",
    "\n",
    "labels = Y[0].to(device)\n",
    "X_list = [torch.from_numpy(X) for X in X_list]\n",
    "X_list = [X.type(torch.FloatTensor).to(device).T for X in X_list]\n",
    "num_nodes = X_list[0].shape[1]\n",
    "A_list_orig = [sparse_mx_to_torch_sparse_tensor(A).to(device) for A in A_list]\n",
    "A_list_normalized = [sparse_mx_to_torch_sparse_tensor(aug_normalized_adjacency(A)).to(device) for A in A_list]\n",
    "A_rho = [get_spectral_rad(adj) for adj in A_list_normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### IDGNN ##################\n",
    "\n",
    "model = bimodel(10, 2, 10, 10, 10, kappa=0.95).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                       lr=0.01)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "Z_0 = torch.zeros(2*10)\n",
    "V_0 = torch.zeros(2*10)\n",
    "eta_1 = 0.9\n",
    "eta_2 = 0.01\n",
    "\n",
    "\n",
    "\n",
    "tr = []\n",
    "tt = []\n",
    "val = []\n",
    "best = 0\n",
    "best_test = 0\n",
    "patient = 0\n",
    "now = time.time()\n",
    "\n",
    "loss_1 = []\n",
    "loss_2 = []\n",
    "acc_1 = []\n",
    "for i in range(2000):\n",
    "        model.train()\n",
    "        model.project(X_list, A_list_normalized, A_rho)\n",
    "        Z_1 = Z_0.to(device).requires_grad_(True)\n",
    "        Z = model(Z_1.view(2, -1))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            re_loss = torch.sum((Z_1 - Z.flatten())**2)\n",
    "\n",
    "        Y_hat = model.classifier(Z.T)\n",
    "\n",
    "        upper_loss = criterion(Y_hat, labels)\n",
    "        lower_loss = ((Z_1-Z.reshape(-1))**2).sum()\n",
    "        \n",
    "        \n",
    "        z_grad = torch.autograd.grad(upper_loss, Z_1, retain_graph=True)[0]\n",
    "        \n",
    "        Z_0 = (1 - eta_1)*Z_0 + eta_1*Z.reshape(-1).detach().cpu()\n",
    "\n",
    "        g_z_grad = torch.autograd.grad(lower_loss, Z_1, retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        hv = torch.inner(g_z_grad,V_0.to(device))\n",
    "        phi_v = torch.autograd.grad(hv, Z_1, retain_graph=True)[0]\n",
    "        V_0 = V_0 - eta_2*phi_v.cpu() + eta_2*z_grad.cpu()\n",
    "\n",
    "        loss = upper_loss - torch.inner(g_z_grad,V_0.to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        if torch.isnan(model.W[0].grad).sum():\n",
    "            import ipdb; ipdb.set_trace()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        acc_train = accuracy(Y_hat, labels).item()\n",
    "\n",
    "        print(f'epoch: {i} loss: {loss.item():.4f} reconstruct loss: {re_loss.item():.4f} acc: {acc_train:.4f}')\n",
    "        loss_1.append(upper_loss.item())\n",
    "        loss_2.append(re_loss.item())\n",
    "        acc_1.append(acc_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = Z_0.view(2,10).T.cpu().detach().numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "ax = sns.heatmap(Z, linewidth=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## TGCN & GRU-GCN ######################\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(iterable):\n",
    "        return iterable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import GConvGRU, TGCN\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, num_hid, node_features, num_layers, num_class):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.recurrent = nn.ModuleList([TGCN(node_features, num_hid) if i==0 else TGCN(node_features, num_hid) for i in range(num_layers) ])\n",
    "        # self.recurrent = nn.ModuleList([GConvGRU(node_features, num_hid) if i==0 else GConvGRU(node_features, num_hid) for i in range(num_layers) ])\n",
    "        \n",
    "        self.linear = torch.nn.Linear(num_hid, num_class)\n",
    "        self.log = nn.LogSoftmax(dim=1)\n",
    "        self.layers = num_layers\n",
    "\n",
    "    def forward(self, X_list, A_list, prev_hidden_state):\n",
    "        x = X_list[0].T\n",
    "        edge_index = A_list[0].coalesce().indices()\n",
    "        edge_weight = A_list[0].coalesce().values()\n",
    "        h = self.recurrent[0](x, edge_index, edge_weight, prev_hidden_state)\n",
    "        for i in range(1, self.layers):\n",
    "            x = X_list[i].T\n",
    "            edge_index = A_list[i].coalesce().indices()\n",
    "            edge_weight = A_list[i].coalesce().values()\n",
    "            h = self.recurrent[i](x, edge_index, edge_weight, h)\n",
    "        return h\n",
    "    \n",
    "    def predict(self, h_0):\n",
    "        y = F.relu(h_0)\n",
    "        y = self.linear(y)\n",
    "        y = self.log(y)\n",
    "        return y\n",
    "    \n",
    "device = 'cuda:7'\n",
    "model = RecurrentGCN(node_features = 10, num_hid = 2, num_layers=len(A_list), num_class=10).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = torch.nn.NLLLoss()\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "accs = []\n",
    "y = Y[0]\n",
    "for epoch in range(2000):\n",
    "    h = None\n",
    "    h = model(X_list, A_list_orig, h)\n",
    "    y_hat = model.predict(h)\n",
    "    cost = loss(y_hat, y.to(device))\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    acc = accuracy_score(y, y_hat.argmax(1).cpu())\n",
    "    accs.append(acc)\n",
    "    print(f'cost: {cost:.4f}, acc {acc:.4f}')\n",
    "    losses.append(cost.item())\n",
    "model.eval()\n",
    "\n",
    "\n",
    "h = None\n",
    "h = model(X_list, A_list, h)\n",
    "y_hat = model.predict(h)\n",
    "acc = accuracy_score(y, y_hat.argmax(1).cpu())\n",
    "print(\"Acc: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h.cpu().detach().numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "ax = sns.heatmap(h, linewidth=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Training loss')\n",
    "plt.plot(losses, label='baseline')\n",
    "plt.plot(loss_1, label='IDGNN')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Training loss')\n",
    "plt.plot(accs, label='baseline')\n",
    "plt.plot(acc_1, label='IDGNN')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6119",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
